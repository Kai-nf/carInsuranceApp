# -*- coding: utf-8 -*-
"""Typo checking

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1myxCjkGAyA1jC6UpysIs3Hf4t_wXQqOM
"""

# Load model directly
from transformers import AutoModel
model = AutoModel.from_pretrained("gotutiyan/gector-roberta-base-5k", torch_dtype="auto")

"""Environment setup"""

!pip install -q "git+https://github.com/gotutiyan/gector"
!pip install -q transformers==4.48.3 accelerate>=0.33.0 pandas scikit-learn python-levenshtein



# Download the verb dictionary required by GECToR (used at inference time)
!mkdir -p data utils
!wget -q -O data/verb-form-vocab.txt \
https://github.com/grammarly/gector/raw/master/data/verb-form-vocab.txt


# Pull the official preprocessing helpers from Grammarly's repo (GECToR format)
!wget -q -O utils/preprocess_data.py \
https://github.com/grammarly/gector/raw/master/utils/preprocess_data.py
!wget -q -O utils/helpers.py \
https://raw.githubusercontent.com/grammarly/gector/master/utils/helpers.py

import pandas as pd


df = pd.read_csv('user_inputs.csv')
assert set(['user_input_plate','user_input_brand','user_input_model','user_input_year',
'expected_brand','expected_model','expected_year','error_type']).issubset(df.columns)
print('\nClass balance (error_type):')
print(df['error_type'].value_counts())

import os, random, numpy as np, torch
SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)
torch.cuda.manual_seed_all(SEED)
os.environ["PYTHONHASHSEED"] = str(SEED)

import re
import numpy as np
from sklearn.model_selection import StratifiedShuffleSplit, train_test_split

# Robust linearization for each row
def to_source(row):
    return f"plate: {row.user_input_plate} brand: {row.user_input_brand} model: {row.user_input_model} year: {row.user_input_year}"

def to_target(row):
    return f"plate: {row.user_input_plate} brand: {row.expected_brand} model: {row.expected_model} year: {row.expected_year}"

src_lines = df.apply(to_source, axis=1).astype(str)
trg_lines = df.apply(to_target, axis=1).astype(str)

# Basic sanity check
assert len(src_lines) == len(trg_lines)

# Train/Dev split (robust fallback)
labels = df['error_type'].astype(str)

try:
    if labels.nunique() > 1 and labels.value_counts().min() >= 2:
        sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)
        idx_train, idx_dev = next(sss.split(df, labels))
    else:
        raise ValueError("Not enough samples per class for stratified split")
except ValueError:
    print("Insufficient samples for specific category, use fallback method - random split")
    idx_train, idx_dev = train_test_split(
        np.arange(len(df)), test_size=0.2, random_state=SEED
    )

src_train, src_dev = src_lines.iloc[idx_train], src_lines.iloc[idx_dev]
trg_train, trg_dev = trg_lines.iloc[idx_train], trg_lines.iloc[idx_dev]

# Write raw files
!mkdir -p data
with open('data/train.src', 'w', encoding='utf-8') as f: f.write('\n'.join(src_train))
with open('data/train.trg', 'w', encoding='utf-8') as f: f.write('\n'.join(trg_train))
with open('data/dev.src',   'w', encoding='utf-8') as f: f.write('\n'.join(src_dev))
with open('data/dev.trg',   'w', encoding='utf-8') as f: f.write('\n'.join(trg_dev))

"""Preprocess to GECToR format"""

# This script tokenizes and builds the tag-friendly format expected by train.py
!python utils/preprocess_data.py \
-s data/train.src -t data/train.trg -o data/train.prep
!python utils/preprocess_data.py \
-s data/dev.src -t data/dev.trg -o data/dev.prep


!head -n 2 data/train.prep

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/gotutiyan/gector.git
# %cd gector

args = [
    "train.py",
    "--train_file", "../data/train.prep",
    "--valid_file", "../data/dev.prep",
    "--save_dir", "../outputs/car-gector",
    "--model_id", "roberta-base",
    "--batch_size", "32",
    "--max_len", "128",
    "--n_epochs", "10",
    "--n_cold_epochs", "2",
    "--lr", "5e-5",
    "--cold_lr", "1e-3",
    "--num_warmup_steps", "0",
    "--label_smoothing", "0.0"
]

cmd = "accelerate launch " + " ".join(args)
print("Running:", cmd)
!$cmd

from transformers import AutoTokenizer
from gector import GECToR, predict, load_verb_dict


model_dir = 'outputs/car-gector/best'
model = GECToR.from_pretrained(model_dir)
tokenizer = AutoTokenizer.from_pretrained(model_dir)
encode, decode = load_verb_dict('data/verb-form-vocab.txt')


# Example: raw noisy record â†’ corrected record
inputs = [
"plate: VBM1234 brand: Toyta model: Vios year: 2019",
"plate: WYX9988 brand: Hondaa model: Civiv year: 2018",
]
corrected = predict(
model, tokenizer, inputs, encode, decode,
keep_confidence=0.2, # bias toward KEEP to reduce overcorrection
min_error_prob=0.0,
n_iteration=5,
batch_size=8,
)
for s, c in zip(inputs, corrected):
  print(f"SRC: {s}\nOUT: {c}\n")